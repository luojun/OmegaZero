# Learning Affordances for Commonsense Knowledge

The following musings are just to get the exploration going at the lowest relevant level: if we are to figure out sensorimotor abilities generally useful in Oz games, then issues of latency, directional alignment, ego localization, metric correspondence, cross-modal correspondence etc must all be dealt with. What is the learning architecture -- components, representations, and algorithms -- needed for a successful incorporation and coordination of all these aspects of general sensorimotor abilities to enable play and learning? That is the question we are wrestling with here. That question, we believe, is also a fundamental part of the broader question about how "commonsense knowledge" may be learned.

The initial articulation below is largely from the perspective of low-level affordances, from the perspective of building from sensorimotor coordination towards the ability to learn more affordances through play. J.J.Gibson's theory of affordance, Jean Piaget's account of sensorimotor development, the idea of General Value Functions (GVFs) from reinforcement learning research, and general robotics and systems considerations are the main perspectives informing our musings here.

## 1. Visual and tactile latency -- time 0: embeddedness

For simplicity of discussion, let's assume there is no delay between motion command and kinesthetic feedback. We may then ask how kinesthetic feedback and visual feedback are correlated to enable estimation of latency of visual feedback. "Latency" may not be the right word here. What matters is the temporal regularity in terms of when an action is taken by the agent and when the impact of that action is perceived by the agent visually. The sense of latency or rather regularity here grounds a general sense for the agent that its action does impact the environment or at least the agent's own body (i.e. its cursor). Similar things can be said about the latency or regularity in tactile feedback. In this sense, the temporal regularity we are concerned about is not unlike the situation with rhythmic finger sucking in babies, wherein the coordinated rhythm of sucking and feeling of being sucked embodies the temporal regularity in question.

To capture the regularity here, we may use one GVF to predict *gross visual motion* based on kinesthetic feedback and use another GVF to predict *gross tactile change* based on kinesthetic feedback.

(Side note: if the action command is also accessible as such, i.e. independent of and prior to the availability of the kinesthetic feedback, we can use GVFs to make similar predictions from action commands rather than from kinesthetic feedback. NB: it is very likely that architecturally speaking higher level or cortical commands, i.e. actions, could be accessible as such, but not lower-level or spinal commands, i.e. reactions.)

Naturally, we could expect these GVFs to predict the *arrival* of visual or tactile feedback. But predicting visual and tactile feedback according to the actual *rhythm* in ego action is probably more important. This is because rhythmic changes in the environment or body under the agent's impact is the best indication that the agent is in control of that part of the environment or its own body.

Here, we can also see how such predictions embody a kind of perceived *invariance*, which is the invariance of the general effectiveness of action regardless of where and how the action is taken. Such GVF predictions and the "nexting" based on such predictions are probably at the core of the agent's sense that it is a little creature appropriately embedded in the Oz world in a way such that it can do something to the Oz world, including its own body.

From such a perspective, we can also see how such GVF predictions may be useful. They are super useful for system self-check: (1) if predictions from action command to gross visual motion (moving the cursor at all if not also the stone held) or gross tactile change (from not touching anything to touching something) fails, it means the agent's control of its body or environment has come apart; more specifically, (2) if predictions from kinesthetic feedback to environmentally rooted feedback fails, it means the agent's body has come apart from its environment (e.g. the agent may be skidding or in total darkness), and (3) less dramatically, if latency between action and gross visual motion increases, the agent is in a position to suspect some sort of computational lag or extra mechanical play was somehow introduced in the overall system. Super useful things for real world robotics!

Finally, it should be noted that one place where the predictions will necessarily be irregular or different is when the agent tries to move against the boundaries of Oz. While this counts as an instance of (2) above, while the unusual GVF predictions may contribute to the agent's sense that it is at the boundaries, a proper treatment of perception of the boundaries of Oz as boundaries will require understanding more capacities to be unpacked below.

## 2. Visual action orientation -- space 1: action-vision directional alignment

With the *temporal* regularity capturing the fundamental embeddedness of the agent in Oz, we can move on to consider the specifically *spatial* aspects of the agent-environment relationship. A first challenge we face here is establishing the *directional* coordination between visual feedback and action.

Let's assume motion command is in robot-base coordinateas, as in a SCARA or Cartesian robot, but the camera axis may undergo rotations relative to the robot-base. To cope with such a change, the agent will need to learn to correlate kinesthetic feedback *direction* and visual feedback *direction*.

To establish the coordination, we could use a GVF that predicts the visual "direction" from the kinesthetic "direction". Such "directions" could be represented with normalized unit vectors. We put "direction" in scare quotes because, as will be illustrated below, at this level of coordination the agent does not yet have an adequate sense of space for these unit vectors to be its mental representation of spatial directions. For now what matters is that the predictive relation between these unit vectors is systematic and can be learned by a GVF.

While the kinesthetic unit vector is easy to obtain from the kinesthetic feedback, which could simply take the form of a two-dimensional vector. The visual unit vector will require a bit of summarizing over the gross visual motion to arrive at an overall direction of that visual motion, which is most likely just visual motion of that cursor under "my" control. The "summarizing" here could be easily engineered by using for example an optical flow estimator. The predicted timing of gross visual motion, based on the temporal regularity treated above, could be used to filter in the precise temporal segment that matters from the estimated optic flow. For an alternative implementation, the GVF could be realized as a neural network that predicts how "my cursor" will move in the visual field given the kinesthetic feedback and given how it has been moving in the visual field. Furthermore, if we adopt a "field" data structure rather than a two-dimensional vector data structure, the GVF could be implicitly realized as the directionally-specific spread of the cursor-based activation in the field. We will return to the potential importance of "field" representation of GVFs again later.

From a first person perspective, the situation we are dealing with is like this: our waving a hand in a particular direction leads us to expect our seeing the hand visually moving in a specific direction. That we have such expectation that link action and perception is why a new pair of glasses make things look weirdly warped. Things look warped because the new glasses altered the optical paths which form part of the underlying systematicity that our predictions have come to rely on. That we are able to quickly adapt to a new pair of glasses means that we have the right kind of plasticity for coping with the underlying shifts. This kind of plasticity is also what allows humans to cope with the more dramatic case of [inverting lens](https://en.wikipedia.org/wiki/George_M._Stratton#Wundt's_lab_and_the_inverted-glasses_experiments).

The kind of invariance that is achieved here is invariance over shifts between the eye or the camera (or rather the optical path, to be precise) and the body base. The plasticity or flexibility that we can build into the GVF predictor will allow us to handle such shifts, which could include camera rotation, *without explicit calibration*.

As with other aspects of coordination or invariance, the boundaries of the Oz world are special. But that predictions do not work normally along some dimensions at the boundary is actually the signature event that tells the agent that it is actually at the boundary.

Another point worth noting is that we are assuming that we could use some hand-crafted cumulants (e.g. the unit vectors) to build the GVF predictor. This is an example of the kind of "generic or commonsense knowledge" that we are for now building into the agent, because, for now, we are doing our exploration [ideally](README.md#ideally) rather than [idealistically](README.md#idealistically). The fundamental limitation with the GVFs is that they assume that the relevant dimensions of interest, or "cumulants", are given. But full-blown play does not assume that. For now, let's play ideally in Oz.

## 3. Visual ego localization -- space 2: where am I, visually speaking?

In addition to directional correspondence, we also need to be concerned about locational correspondence: *where* in the visual field the impact of "my" current activity will be. However, the sense of "where" here requires a bit of analysis. While it may be obvious that the "where" could be represented by a pair of coordinate values, that will require choosing an origin in addition to the dimensions of the direction. This will force a full explicit coordinate system as a precondition for representation. While we are used to utilizing explicit coordinate systems in conventional robotics, the reliance on them actually necessitates precise measurement and explicit calibration, which are typically taken care of through engineering practice beyond the agent's own control. That in turn makes the solution brittle and hard to generalize. For this reason, while conceptually we may still think of the "where am I" ego-localization in terms of GVF style prediction over coordinates, we probably should choose a field sort of representation, a field with a single or possibly multiple activation peaks for where I could possibly be with respect to the visual field. Such a design avoids the explicit choice of a coordinate system. Insofar as the field coincides with the visual field, we may implicitly rely on the camera-centered coordinates.

Now, the point of "where am I" in the visual field is so that the agent could know or predict the specific locus of its action in the visual scene, i.e. where in the visual field is "my" motion causing localized visual changes. The sort of invariance achieved here is that regardless of where I am actually, I am always able to localize myself through where my action is in the ego-centric or "camera-centered" coordinates. This gives the agent a certain degree of spatial differentiation between here and eleswhere. The basis of this here vs. elsewhere differentiation is the predictions about where ego-centrically "my cursor" will be or could be next. With this sort of localization, "my" actions will now have an ego anchor. My actions will now be taken from *somewhere*. (Side note: The situation with Oz is somewhat different from the case of a human person because our eyes are in our heads, rather than outside of our body. Thus the visual world for us humans is centered around our heads, which is where the "ego" arguably is. But the locus of the Oz agent is arguably where its cursor is, with the all-seeing camera being in a bird's eye position outside of not only the cursor body but also the Oz flatland.)

Note that this sort of visual ego localization could be supported by the directional alignment discussed above. The directional alignment will help us to tell which moving cursor is me, based on the predicted and the actual direction of the movements of the cursors. Thus, ego localization could be helped by attaching direction to a visually disturbed area as well as with the help of timing of the disturbance (temporal regularity). In the reverse direction, the visual ego localization could also support directional alignment through localizing the visual direction change to where I am. Insofar as this sort of mutual support is about using the past to predict the future, we do not have a vicious dependency here. Instead, it is more like ongoing future-oriented fusion of multiple constraints or sources of information.

It might be suggested that ego localization should be considered with respect to the boundaries of the Oz flatland. In other words, it may be suggested that the agent could use the boundaries of the Oz flatland to define the reference frame it relies on rather than implicitly depends on the camera coordinates. But relationship to the boundary is indeed both different from and unnecessary for the visual ego localization. For example, even in a boundless field in the wild, we humans still have no problem localizing ourselves at where we are at the time, and localize our actions and their effects *ego-centrically* to where we are. The here vs. elsewhere differentiation is spatial differentiation without absolute coordinates or boundary references. The boundaries will be more relevant when we consider metric correspondence and distance next.

## 4. How far to travel and when to arrive where -- space 3: metric correspondence

Coming up next is the affordance for or a visual sense of how far it is to travel across the visual field and how long it is going to take to arrive somewhere. This one requires integration of visual orientation and visual localization, which are discussed in the previous two sections. It further requires the metric correspondence between the two independently controllable action/motion dimensions (i.e. the two dimensions of the speed command that an Oz agent uses) and the visible changes of ego location along a particular direction. Implementation-wise we could use yet another field, centered around where the agent is, and spreads out according to how much time it takes to arrive at any other points in the field.

The invariance in question here is with respect to change in visual scaling, due to the scaling aspects of the camera configuration (zooming, distance from the camera lens to the Oz flatland etc.). The basic idea here is that visual distance or distance in the visual field has to be measured by the actual travel of the agent (by foot so to speak). More rigorously speaking, the visual field has no metric expanse associated with it until a metric correspondence between locomotion and visual change is established through the agent moving its cursor in specific ways in the Oz flatland *and* making accurate enough predictions about where the cursor will be next. Once the correspondence is established, the agent will be able to estimate how far it takes for it to travel across a certain span in the visual field, regardless of the visual scaling. The *units* of the metric correspondence could be based on the *cycles* of temporally rhythmic regularity in the Oz agent's action or at least be based on the *count* of incremental unit-sized ego-motion steps, as the Oz cursor moves along a particular direction to a new ego location.

With such a correspondence established, the boundaries, given the special cursor behavior there, may also become boundaries in the visual space of the agent. Starting from anywhere, the agent may have a sense of how long in terms of time and distance it is going to take for it to arrive in a particular direction at a particular spot at the boundary. Moreover, the agent may also have a sense that at a boundary, metric correspondence in a particular direction will fail. The boundaries may then start to serve as landmarks for localization or as anchors of reference frames.

Now, presumably visual discontinuities such as where the board ends and the background starts, such as how lines intersect and end, such as how one stone occludes part of the board, can all serve as points of interest, the distance to which from the ego agent may now be visually estimated. The importance of such visual discontinuities is something that the agent could learn to be sensitive to, based on the sorts of affordances associated with them. But for now, we are not explicitly treating these aspects of vision or visual perception of affordances yet. We will comment on this more when we deal with the spatial distribution of visible features such as "stoneness" later.

## 5. What do I feel now? -- tactile feedback

So far, other than the tactile latency discussed earlier, we have only treated visual feedback in detail. Obviously, we also need to consider the tactile feedback beyond just tactile latency which is concerned with mere tactile change due to a "press" or "touch" action. Let's take a look at the specific quality content of tactile feedback.

Note first that since tactile feedback in Oz, unlike in the real human case, takes a point form. By the design of Oz, the tactile feedback is determined from sampling the point of the environment at the center of the cursor. It thus makes no sense to talk about tactile orientation. But the point feedback will come with differentiating quality such that pressing on the board feels different from pressing on the background, pressing on a stone feels different from pressing on the board, and pressing on a white stone feels the same as pressing on a black stone. (Note that Oz in its current version of implementation does not support one agent getting tactile feedback about another agent. The agents and their cursor embodiments do not afford being touched.) For now, it is the temporal course of tactile feedback that matters. In the simplest and most basic form, the prediction here would just be if I press, I will feel a certain quality and if I continue to press, I will most likely continue to feel that same quality. If I am to let go, I will then feel nothing. If I am to press again, I will feel that same quality again.

In this purest form, the point tactile feedback is not located with respect to anything in the Oz flatland other than the immediate ego, or more specifically the point that is the center of the cursor. As the ego moves elsewhere in the Oz environment, the content or quality of the tactile feedback will change. In the more elaborate version of tactile feedback prediction, the agent may consider not only its immediate press action and past feeling, but also its ego motion and visual input. These considerations get us to the next two topics.

## 6. Where will I feel what? -- tactile map

If we correlate how I feel and how I move, we could start building out a tactile map of the environment. Given adequate kinesthetic feedback that supports good enough dead reckoning, the map could be based solely on the temporal correlation of what I felt at where I was. What is required is of the agent is the ability to remember how it has moved in the short term and also retain in a certain form of long term memory the actual content of the tactile feature map.

Representation-wise, we could again use a GVF field, possibly realized with convolutional filters spanning both spatial and temporal dimension or a CNN+RNN composition. The neural network will combinethe tactile feature map with current tactile feedback, current location in the tactile map, and current motion to predict tactile feedback at the next moment. This capacity could serve as the basis for the agent's sense that if I am to move further in that way, I will stop feeling the board and start feeling the background. Or if I am to go back to where I just came from, I will start feeling the stone again.

Equipped with accurate kinesthetic feedback and strong memory capacity, a "blind" Oz agent could still go a long way in the Oz flatland and possibly learn to play all the games a normal sighted Oz agent could learn to play, so long as the environment does not change too fast in these games for the tactile map to be updated in time.

## 7. The looks of how things feel -- cross-modal visual-tactile field

In contrast to a "blind" Oz agent, a normal Oz agent is blessed with the opportunity of learning to correlate visual input with tactile intput to form a visual-tactile field. From such a cross-modal field, the agent will be able to tell, from just the looks of things, where how things will feel. Moreover, because the agent is already in a place to tell visually how far it is going to take for it to get somewhere from where it is, it could also tell how long it's going to take to get somewhere to feel something.

The main piece of implementation needed for such a visual-tactile field is the cross-modal spatial alignment between tactile feedback and visual feedback. This could be done by combining a visual field and a separate tactile field. Alternatively, we could exploit the visual field along with the directional and metric correspondences that are already supported to place the felt tactile qualities within the visual field. Regardless of such implementation detail, what we need is something like a cross-modal GVF field wherein the predictions about how things feel could be at least partly based on how things look and vice versa.

## 8. Where are the stones? -- stoneness as a particular kind of spatially distributed affordance

From the perspective of a game as a game, the stones are defined in their functional roles in the game. Their essence in the game lies in their being the pieces we use in particular ways according to the rules of the game. However, before we get to that level of abstraction, let's stick to the physical aspects of the stones in Oz. After all, our goal is to learn to play games using these "physical" stones in our virtual Oz flatland. From that perspective, stones are the black or white round physical pieces that the agent can play with. That they can be physically played with in particular ways is what makes them stones and give them their "stoneness".

A first thing to note about the stones in Oz is that color alone does not differentiate stoneness from the rest of the environment. For example, the lines on the board can be of a similarly black color as the black stones. Thus, the agent cannot rely on color alone to pick out stones from the environment. What the agent can do is discover how disc-shaped black or white patches could be picked up, dropped down, and even carried along. The delineation of a stone from the rest of the environment could happen in three mutually supportive ways. First, the felt tactile quality changes at the edge of the disc shape. Second, as the cursor itself moves over a stone, a blue patch (the cursor) will come to occlude and disocclude a black or a white patch (the stone). Third, the whole patch of black or white could be moved if a suitable sequence of actions happen to be issued. These three aspects can be adequately coordinated to ground a sense of stoneness for the Oz agent.

Implementing such a sense of stoneness could be yet another kind of GVF that combines color, tactile, location, movement, and action inputs to predict future color, tactile, location and motion that are particular to stones. Such a GVF will be able to make predictions as to how a piece of stone will move or feel, without representing its individuality (i.e. without so to speak "tracking" it as a unique individual stone). The prediction here is still at the level of generic affordances of the stone. As stones becomes something of general interest to the agent, possibly because it can be much more easily moved around than other things in the Oz flatland, the agent may learn a GVF *field* that predicts the spatial distribution of stoneness as a form of affordance. Places where there are currently stones versus places where there are not could become important in ways like places where there is food or water versus where there is none.

## 9. Moving stones -- from play to purpose

Intuitively, when given the right learning architecture as well as the corresponding idealizations, the agent should be able to learn all of the above through unsupervised or self-supervised predictive learning. But the actual learning process will likely require a lot of playing in the environment.

We are now at a point where the agent could begin to play with the stones qua stones, qua what a stone can do and what the agent can do with it in the environment in a mostly physical sense. We said that to arrive at this level of complexity probably requires a lot of play. Such play could lead to the discovery of much regularity and lead to the acquisition of much ability. Such play could also appear to be highly willful or highly desirable insofar as the agent is persistent in its repetition of the specific play activity. Through repetition, the possibility is open for the actual result being taken to be the post hoc purpose of the action. The playing with the stones for example does not yet involve organizing actions towards a purpose, such as moving a stone to a particular point where there is currently no stone. This remains so until the agent somehow starts to want to *repeat an action-result pair* that first came about through mere play. The desire to repeat an action-result pair may have to do with a largerly activity. For example, by repeatedly moving a couple of black stones to where a white stone is, the agent may discover that the white stone can be hidden completely by the black stones, then by repeating this all over the Oz environment, a new post hoc purpose of making all white stones disappear may be instituted. But in the meantime, the originally post hoc purpose of moving a stone now has a purpose in the larger context of hiding the white stones. The result in the original stone-moving repetition may then become purpose prospectively rather than retrospectively.

At this point, two things should be clarified. First, the willful or affective aspect of the overall architecture certainly matters. The intertwining of cognitive and affective aspects is in a way a given for our line of exploration. However, we may choose to limit the scope of our actual exploration by simply positing some affective mechanism that is needed to work in coordination with the more cognitive aspects of the mechanism. That is, we may choose not to tell an epigenetic story about the affective mechanisms at all. This is another place where we are doing things ideally rather than idealistically. Oz after all is not a world that is rich enough to support the rise of life, the rise of value for survival, or the rise of ethical judgements.

Second, it is not accidental that we choose the ability to purposefully move stones as the last stage of this initial phase of our exploration, i.e. exploration in the epigenesis of affordance through sensorimotor coordination. After all, our over all goal is learning to play the game using the stones. Towards that goal, the shift from purposeless play with the stones into purposeful play with the stones is highly significant, because it gives a foothold for further purpose-oriented behaviors that could be established through higher level purposeless play, i.e. play in which the lower level moves are purposeful but the higher level organization could remain purposeless until a certain willed or desired repetition of results takes hold of the agent.

(Side note: Although purposefully moving stones around is crucial for where we are going -- for getting to learn to play the games, the earliest form of post hoc purposeful action in the Oz environment could be much simpler. It could take the form of moving the cursor around purposefully, through first wanting to repeat an earlier resultant state, such as a resultant state that is as simple as seeing one's cursor moving in the visual field or feeling a stone by pressing down. In short, this whole play-to-purpose transition first through repetition and then through incorporation of repeatition under higher order activity structure happens a lot earlier before the agent is capable of meaningfully manipulate stones.)

## ... and so on ...
 
We have roughly charted the lay of the land for building up from primitive sensorimotor abilities all the way to a level of complexity where stones could be handled with by the agent on the way towards purpose-engendering play. The sense is that such complexity is close to be within the reach of contemporary AI techniques and computational capacity. What is needed is for us researchers to walk the land, to blaze new paths, to fashion new techniques, so that we may be able to both build *and* understand the capacity to play and to further learn through play. What is *not* going to work, i.e. what is not going to allow us to succeed at arriving at learning by playing, would be to trivialize the crucial parts of the complexity charted here such that our AI system or robots will just "work" in an engineering sense. Indeed, they may work and win Go games and drive cars, but they won't be able to [play in Oz](play.md).

